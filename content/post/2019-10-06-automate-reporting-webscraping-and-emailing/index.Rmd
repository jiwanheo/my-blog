---
title: 'Automate Reporting: Webscraping and Emailing'
author: Jiwan Heo
date: '2019-10-06'
tags:
  - webscraping
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style>
p.caption {
  font-size: 0.75em;
  font-style: italic;
  color: black;
  background-color: #f2f2f2;
}
</style>

Hey, just wanted to share how you can automate the reporting process of pulling information from a website and emailing it to people or to yourself. You can even schedule this to run everyday at a certain time, which is neat.

The process itself is pretty simple. You first figure out how the website presents the information you're looking for, scrape it, and edit it. The workflow I'm suggesting in this post is as follows:

1. [Scrape](#scrape)
2. [Email](#email)
3. [Run this every morning](#scheduler) 

## Scrape

```{r fig.cap = "Marketwatch.com front page", echo = FALSE}
knitr::include_graphics("lets_scrape.png")
```

We're gonna pull from marketwatch.com, the good old Dow, S&P and 4 other familiar metrics at the top left corner.

As usual, I'm using tidyverse to clean up data. The new packages I'm using today are `rvest`, for webscraping, `mailR` for emailing.

```{r message = FALSE}
library(tidyverse)
library(rvest)
```

If we go back to chrome and press `F12` on PC, or `Crtl` `Option` `J` on Mac, it'll take you to developers mode. 

```{r fig.cap = "Important features", echo = FALSE}
knitr::include_graphics("important_info.png")
```

Make sure you're on the "Elements" tab. Here, you want to find the HTML element of the things you want to pull. You can either do this by clicking the cursor Icon at the top, and clicking the object, or unnest your way down the HTML code on the right side.

Now to scrape this table, we can use `.` to call the classes of the `table` element (3 of them "table", "table--primary", "align--right"). Kinda confusing how we're calling a `table` element that has the class of "table", but yea. It's super useful how you can call multiple classes of objects to narrow down the search.

```{r}
webpage <- read_html("https://www.marketwatch.com/")

webpage %>% 
  html_node("table.table.table--primary.align--right") 
```

Taking it a step further, we're going to pull a `child` element of the table, `tbody`, which has the classes of `row-hover`, and `markets__group`. As you can imagine, `tbody` is short for "table body" 

```{r}
markets <- webpage %>% 
  html_node(".table.table--primary.align--right > 
            tbody.row-hover.markets__group") 
```

```{r fig.cap = "Construction of the HTML table", echo = FALSE}
knitr::include_graphics("table_content.png")
```

If we look at the structure of the `tbody`, there are 6 indexes that we're interested in, each one of them being `tr`, short for table row. (Dow, S&P 500, ...) Then each row is further divided into `table__cell`, `table__cell symbol`, `table__cell price`, and so on. We're going to just pull symbol and price. Now, let's pull all texts from this table.

```{r}
markets %>% 
  html_text() 
```

This is too messy, so I'm going to use `str_squish`, which removes obviously unnecessary empty spaces including new lines. Also, I'm going to remove the commas, just to make it easier for myself to do regex later to pull these information.

```{r}
text_markets <- markets %>% 
  html_text() %>% 
  str_squish() %>% 
  str_remove("\\,")

text_markets
```

If you read my [first post](https://jiwanheo.rbind.io/blog/2019-07-28-nba-twitter-sentiment-2019-offseason/), I used the `tidytext` package to do some sentiment analysis. I'm going to use it again to do some text wrangling. First, I need to unnest the sentence, which means I'm going to break up every single word into a row in a data frame. One inconveniet thing that I couldn't get around was the fact that the `unnest_tokens` function doesn't take anything other than a `data.frame` object as input, so I use the `tibble` function first.

```{r}
library(tidytext)

tidy_text_df <- tibble(text = text_markets) %>% 
  unnest_tokens(word, text)

tidy_text_df
```

Brutal manual labour to deal with "S" "P" "500". 

```{r}
tidy_text_df <- tidy_text_df[-c(6, 7),]
tidy_text_df[5,] <- "S&P 500"
```

This is a little regex to filter out those words that start with a digit, AKA everything excluding our markets. This returns a list of True and False.

```{r}
char_test <- str_detect(tidy_text_df$word, "^\\d")
char_test
```

Here's a neat trick. `!` means "not", so it now assigns True, to the market names. I can use this list of booleans to index a dataframe. (in my case, rows)

```{r}
exchanges <- tidy_text_df[!char_test,]
exchanges
```

Now, I couldn't think of a better way to filter for prices (closing value) of the exchanges. So I inefficiently ran a loop to see detect the market names from `exchanges`, and saved the next row as the corresponding price.

```{r}
prices <- integer()

# Adding the pre-existing vector with the new element every time
for (i in 1:length(tidy_text_df$word)) {
  if (tidy_text_df[i,]$word %in% as.vector(exchanges$word)) {
    prices <- c(prices, tidy_text_df[i +1,]$word)
  }
}

prices
```

Putting it all together, we have a data frame of market names and closing prices.

```{r}
final_df <- data.frame(exchange = exchanges, price = prices)

final_df
```

## Email

OK, so we succesfully gathered and prepared the information we wanted. Let's look at how we can email using the `mailR` package. 

Warning! This package was built as a wrapper around Apache Commons Email, which runs on Java Mail API. So you're going to have to have the same version of Java, as your R (x86 or x64). I had some trouble configuring this, but eventually found a solution after searching on stackoverflow. You have to set the JAVA_HOME Path, which basically tells your computer where to look for Java. You can do this in R with 

```{r}
# Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jdk1.8.0_221\\jre')
```

Notice how it looks for Java in "Program Files" folder, the 64-bit version of Java, which is the same version that my R runs in.

We can go ahead with the `mailR` package now. You'll need to download the `rJava` package as well, which I spent more time than I'd like to admit trying to make it work. Recall the `final_df` we created earlier. It turns out that I can't straight up write a data.frame object in the email. To get around this, I just wrote a csv and attached it to the email.

```{r message = FALSE}
# install.packages("rJava)
# install.packages("mailR")

library(rJava)
library(mailR)
```

```{r}
sender <- "jiwanheo123@gmail.com"
recipients <- c("jiwanheo123@gmail.com", "jheo3565@gmail.com")
attachment <- "C:/Users/jiwan/Desktop/rProjects/blog_jiwanheo/content/blog/2019-10-06-automate-reporting-webscraping-and-emailing/market_watch_csv.csv"
market_watch_csv <- write_csv(final_df, attachment)

# send.mail(from = sender,
#           to = recipients,
#           subject = "Marketwatch Data Pull",
#           body = "This is the latest marketwatch pull",
#           smtp = list(host.name = "smtp.gmail.com", port = 465,
#                       user.name = "jiwanheo123@gmail.com",
#                       passwd = "None of your concerns yo", ssl = TRUE),
#           attach.files = attachment,
#           authenticate = TRUE,
#           send = TRUE)
```

The results look like this.

```{r fig.cap = "Downloadable csv file", echo = FALSE}
knitr::include_graphics("email1.png")
```

If that didn't work, you may have to give [less secupre app access](https://myaccount.google.com/lesssecureapps?pli=1) on Google.

## Scheduler

Finally, if you write the above into a nice R script, you can have your computer run the script automatically, with either the built-in task scheduler, or the `taskscheduleR` in R. 

```{r}
# library(taskscheduleR)
# myscript <- system.file("extdata", "scrape_and_email.R", package = "taskscheduleR")

# taskscheduler_create(taskname = "Marketwatch_data_scrape", rscript = myscript, 
#                     schedule = "DAILY", starttime = "09:00", 
#                     startdate = format(Sys.Date()+1, "%d/%m/%Y"))
```

I didn't save this process in a single script, so I commented out the scheduler step, but I think you get the idea. The above will run what we just covered so far, every morning at 9 a.m. starting tomorrow.

There you have it guys, hope you learned something, and maybe you can implement this in your professional life. 